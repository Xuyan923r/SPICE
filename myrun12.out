nohup: ignoring input
Model_abbr: myrun12
save_path: myrun12_questioner_v1
RUN_ID=1764576683534987669
vLLM services started with RUN_ID=1764576683534987669
Start training questioner: storage/models/Qwen3-4B-Base -> myrun12_questioner_v1
Questioner outputs will be dumped to: /data/yexuyan/SPICE/storage/questioner_outputs/myrun12_questioner_v1
INFO 12-01 08:11:27 [__init__.py:244] Automatically detected platform cuda.
INFO 12-01 08:11:27 [__init__.py:244] Automatically detected platform cuda.
INFO 12-01 08:11:27 [__init__.py:244] Automatically detected platform cuda.
INFO 12-01 08:11:27 [__init__.py:244] Automatically detected platform cuda.
INFO 12-01 08:11:28 [__init__.py:244] Automatically detected platform cuda.
/home/yexuyan/miniconda3/envs/r0/lib/python3.10/site-packages/stopit/__init__.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/home/yexuyan/miniconda3/envs/r0/lib/python3.10/site-packages/stopit/__init__.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/home/yexuyan/miniconda3/envs/r0/lib/python3.10/site-packages/stopit/__init__.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/home/yexuyan/miniconda3/envs/r0/lib/python3.10/site-packages/stopit/__init__.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
2025-12-01 08:11:32,881	INFO worker.py:1879 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
[init] Loading model...
INFO 12-01 08:11:37 [config.py:823] This model supports multiple tasks: {'classify', 'embed', 'reward', 'score', 'generate'}. Defaulting to 'generate'.
INFO 12-01 08:11:37 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=16384.
[init] Loading model...
INFO 12-01 08:11:37 [config.py:823] This model supports multiple tasks: {'embed', 'score', 'reward', 'generate', 'classify'}. Defaulting to 'generate'.
[init] Loading model...
INFO 12-01 08:11:37 [config.py:823] This model supports multiple tasks: {'embed', 'generate', 'classify', 'reward', 'score'}. Defaulting to 'generate'.
[init] Loading model...
INFO 12-01 08:11:38 [config.py:823] This model supports multiple tasks: {'generate', 'reward', 'score', 'classify', 'embed'}. Defaulting to 'generate'.
INFO 12-01 08:11:38 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 12-01 08:11:38 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 12-01 08:11:38 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 12-01 08:11:39 [core.py:455] Waiting for init message from front-end.
INFO 12-01 08:11:39 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='storage/models/Qwen3-4B-Base', speculative_config=None, tokenizer='storage/models/Qwen3-4B-Base', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=storage/models/Qwen3-4B-Base, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
WARNING 12-01 08:11:39 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fa8daabeec0>
INFO 12-01 08:11:39 [core.py:455] Waiting for init message from front-end.
INFO 12-01 08:11:39 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='storage/models/Qwen3-4B-Base', speculative_config=None, tokenizer='storage/models/Qwen3-4B-Base', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=storage/models/Qwen3-4B-Base, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-01 08:11:39 [core.py:455] Waiting for init message from front-end.
INFO 12-01 08:11:39 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='storage/models/Qwen3-4B-Base', speculative_config=None, tokenizer='storage/models/Qwen3-4B-Base', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=storage/models/Qwen3-4B-Base, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 12-01 08:11:39 [core.py:455] Waiting for init message from front-end.
INFO 12-01 08:11:39 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='storage/models/Qwen3-4B-Base', speculative_config=None, tokenizer='storage/models/Qwen3-4B-Base', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=storage/models/Qwen3-4B-Base, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
WARNING 12-01 08:11:39 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f0f1c986e60>
WARNING 12-01 08:11:39 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f4b52fe6e00>
WARNING 12-01 08:11:39 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f06adb02d70>
INFO 12-01 08:11:39 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 12-01 08:11:39 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 12-01 08:11:39 [gpu_model_runner.py:1595] Starting to load model storage/models/Qwen3-4B-Base...
INFO 12-01 08:11:40 [gpu_model_runner.py:1600] Loading model from scratch...
INFO 12-01 08:11:40 [cuda.py:252] Using Flash Attention backend on V1 engine.
INFO 12-01 08:11:40 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 12-01 08:11:40 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 12-01 08:11:40 [gpu_model_runner.py:1595] Starting to load model storage/models/Qwen3-4B-Base...
Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
INFO 12-01 08:11:40 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 12-01 08:11:40 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 12-01 08:11:40 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 12-01 08:11:40 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 12-01 08:11:40 [gpu_model_runner.py:1595] Starting to load model storage/models/Qwen3-4B-Base...
INFO 12-01 08:11:40 [gpu_model_runner.py:1595] Starting to load model storage/models/Qwen3-4B-Base...
INFO 12-01 08:11:40 [gpu_model_runner.py:1600] Loading model from scratch...
INFO 12-01 08:11:40 [gpu_model_runner.py:1600] Loading model from scratch...
INFO 12-01 08:11:40 [cuda.py:252] Using Flash Attention backend on V1 engine.
INFO 12-01 08:11:40 [gpu_model_runner.py:1600] Loading model from scratch...
INFO 12-01 08:11:40 [cuda.py:252] Using Flash Attention backend on V1 engine.
INFO 12-01 08:11:40 [cuda.py:252] Using Flash Attention backend on V1 engine.
Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[36m(pid=3026700)[0m WARNING 12-01 08:11:36 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
[36m(Runner pid=3026700)[0m {
[36m(Runner pid=3026700)[0m   "data": {
[36m(Runner pid=3026700)[0m     "train_files": "/data/yexuyan/SPICE/storage/datasets",
[36m(Runner pid=3026700)[0m     "val_files": "/data/yexuyan/SPICE/storage/datasets",
[36m(Runner pid=3026700)[0m     "prompt_key": "text",
[36m(Runner pid=3026700)[0m     "answer_key": "id",
[36m(Runner pid=3026700)[0m     "context_key": "text",
[36m(Runner pid=3026700)[0m     "image_key": "images",
[36m(Runner pid=3026700)[0m     "max_prompt_length": 4096,
[36m(Runner pid=3026700)[0m     "max_response_length": 2048,
[36m(Runner pid=3026700)[0m     "rollout_batch_size": 512,
[36m(Runner pid=3026700)[0m     "val_batch_size": 1024,
[36m(Runner pid=3026700)[0m     "format_prompt": null,
[36m(Runner pid=3026700)[0m     "override_chat_template": null,
[36m(Runner pid=3026700)[0m     "shuffle": true,
[36m(Runner pid=3026700)[0m     "seed": 1,
[36m(Runner pid=3026700)[0m     "max_pixels": 4194304,
[36m(Runner pid=3026700)[0m     "min_pixels": 262144,
[36m(Runner pid=3026700)[0m     "filter_overlong_prompts": false
[36m(Runner pid=3026700)[0m   },
[36m(Runner pid=3026700)[0m   "worker": {
[36m(Runner pid=3026700)[0m     "hybrid_engine": true,
[36m(Runner pid=3026700)[0m     "actor": {
[36m(Runner pid=3026700)[0m       "strategy": "fsdp",
[36m(Runner pid=3026700)[0m       "global_batch_size": 16,
[36m(Runner pid=3026700)[0m       "micro_batch_size_per_device_for_update": 2,
[36m(Runner pid=3026700)[0m       "micro_batch_size_per_device_for_experience": 8,
[36m(Runner pid=3026700)[0m       "max_grad_norm": 1.0,
[36m(Runner pid=3026700)[0m       "clip_ratio_low": 0.2,
[36m(Runner pid=3026700)[0m       "clip_ratio_high": 0.3,
[36m(Runner pid=3026700)[0m       "clip_ratio_dual": 3.0,
[36m(Runner pid=3026700)[0m       "ppo_epochs": 1,
[36m(Runner pid=3026700)[0m       "padding_free": true,
[36m(Runner pid=3026700)[0m       "ulysses_sequence_parallel_size": 1,
[36m(Runner pid=3026700)[0m       "use_torch_compile": true,
[36m(Runner pid=3026700)[0m       "model": {
[36m(Runner pid=3026700)[0m         "model_path": "/data/yexuyan/SPICE/storage/models/Qwen3-4B-Base",
[36m(Runner pid=3026700)[0m         "tokenizer_path": "/data/yexuyan/SPICE/storage/models/Qwen3-4B-Base",
[36m(Runner pid=3026700)[0m         "override_config": {},
[36m(Runner pid=3026700)[0m         "enable_gradient_checkpointing": true,
[36m(Runner pid=3026700)[0m         "trust_remote_code": false,
[36m(Runner pid=3026700)[0m         "freeze_vision_tower": false
[36m(Runner pid=3026700)[0m       },
[36m(Runner pid=3026700)[0m       "optim": {
[36m(Runner pid=3026700)[0m         "lr": 1e-06,
[36m(Runner pid=3026700)[0m         "betas": [
[36m(Runner pid=3026700)[0m           0.9,
[36m(Runner pid=3026700)[0m           0.999
[36m(Runner pid=3026700)[0m         ],
[36m(Runner pid=3026700)[0m         "weight_decay": 0.01,
[36m(Runner pid=3026700)[0m         "strategy": "adamw",
[36m(Runner pid=3026700)[0m         "lr_warmup_ratio": 0.0,
[36m(Runner pid=3026700)[0m         "min_lr_ratio": null,
[36m(Runner pid=3026700)[0m         "warmup_style": "constant",
[36m(Runner pid=3026700)[0m         "training_steps": -1
[36m(Runner pid=3026700)[0m       },
[36m(Runner pid=3026700)[0m       "fsdp": {
[36m(Runner pid=3026700)[0m         "enable_full_shard": true,
[36m(Runner pid=3026700)[0m         "enable_cpu_offload": false,
[36m(Runner pid=3026700)[0m         "enable_rank0_init": true,
[36m(Runner pid=3026700)[0m         "use_orig_params": false,
[36m(Runner pid=3026700)[0m         "torch_dtype": null,
[36m(Runner pid=3026700)[0m         "fsdp_size": -1,
[36m(Runner pid=3026700)[0m         "mp_param_dtype": "bf16",
[36m(Runner pid=3026700)[0m         "mp_reduce_dtype": "fp32",
[36m(Runner pid=3026700)[0m         "mp_buffer_dtype": "fp32"
[36m(Runner pid=3026700)[0m       },
[36m(Runner pid=3026700)[0m       "offload": {
[36m(Runner pid=3026700)[0m         "offload_params": true,
[36m(Runner pid=3026700)[0m         "offload_optimizer": true
[36m(Runner pid=3026700)[0m       },
[36m(Runner pid=3026700)[0m       "global_batch_size_per_device": -1,
[36m(Runner pid=3026700)[0m       "disable_kl": false,
[36m(Runner pid=3026700)[0m       "use_kl_loss": true,
[36m(Runner pid=3026700)[0m       "kl_penalty": "low_var_kl",
[36m(Runner pid=3026700)[0m       "kl_coef": 0.01
[36m(Runner pid=3026700)[0m     },
[36m(Runner pid=3026700)[0m     "critic": {
[36m(Runner pid=3026700)[0m       "strategy": "fsdp",
[36m(Runner pid=3026700)[0m       "global_batch_size": 256,
[36m(Runner pid=3026700)[0m       "micro_batch_size_per_device_for_update": 4,
[36m(Runner pid=3026700)[0m       "micro_batch_size_per_device_for_experience": 16,
[36m(Runner pid=3026700)[0m       "max_grad_norm": 1.0,
[36m(Runner pid=3026700)[0m       "cliprange_value": 0.5,
[36m(Runner pid=3026700)[0m       "ppo_epochs": 1,
[36m(Runner pid=3026700)[0m       "padding_free": false,
[36m(Runner pid=3026700)[0m       "ulysses_sequence_parallel_size": 1,
[36m(Runner pid=3026700)[0m       "model": {
[36m(Runner pid=3026700)[0m         "model_path": null,
[36m(Runner pid=3026700)[0m         "tokenizer_path": null,
[36m(Runner pid=3026700)[0m         "override_config": {},
[36m(Runner pid=3026700)[0m         "enable_gradient_checkpointing": true,
[36m(Runner pid=3026700)[0m         "trust_remote_code": true,
[36m(Runner pid=3026700)[0m         "freeze_vision_tower": false
[36m(Runner pid=3026700)[0m       },
[36m(Runner pid=3026700)[0m       "optim": {
[36m(Runner pid=3026700)[0m         "lr": 1e-06,
[36m(Runner pid=3026700)[0m         "betas": [
[36m(Runner pid=3026700)[0m           0.9,
[36m(Runner pid=3026700)[0m           0.999
[36m(Runner pid=3026700)[0m         ],
[36m(Runner pid=3026700)[0m         "weight_decay": 0.01,
[36m(Runner pid=3026700)[0m         "strategy": "adamw",
[36m(Runner pid=3026700)[0m         "lr_warmup_ratio": 0.0,
[36m(Runner pid=3026700)[0m         "min_lr_ratio": null,
[36m(Runner pid=3026700)[0m         "warmup_style": "constant",
[36m(Runner pid=3026700)[0m         "training_steps": -1
[36m(Runner pid=3026700)[0m       },
[36m(Runner pid=3026700)[0m       "fsdp": {
[36m(Runner pid=3026700)[0m         "enable_full_shard": true,
[36m(Runner pid=3026700)[0m         "enable_cpu_offload": false,
[36m(Runner pid=3026700)[0m         "enable_rank0_init": false,
[36m(Runner pid=3026700)[0m         "use_orig_params": false,
[36m(Runner pid=3026700)[0m         "torch_dtype": null,
[36m(Runner pid=3026700)[0m         "fsdp_size": -1,
[36m(Runner pid=3026700)[0m         "mp_param_dtype": "bf16",
[36m(Runner pid=3026700)[0m         "mp_reduce_dtype": "fp32",
[36m(Runner pid=3026700)[0m         "mp_buffer_dtype": "fp32"
[36m(Runner pid=3026700)[0m       },
[36m(Runner pid=3026700)[0m       "offload": {
[36m(Runner pid=3026700)[0m         "offload_params": false,
[36m(Runner pid=3026700)[0m         "offload_optimizer": false
[36m(Runner pid=3026700)[0m       },
[36m(Runner pid=3026700)[0m       "global_batch_size_per_device": -1
[36m(Runner pid=3026700)[0m     },
[36m(Runner pid=3026700)[0m     "ref": {
[36m(Runner pid=3026700)[0m       "strategy": "fsdp",
[36m(Runner pid=3026700)[0m       "fsdp": {
[36m(Runner pid=3026700)[0m         "enable_full_shard": true,
[36m(Runner pid=3026700)[0m         "enable_cpu_offload": true,
[36m(Runner pid=3026700)[0m         "enable_rank0_init": true,
[36m(Runner pid=3026700)[0m         "use_orig_params": false,
[36m(Runner pid=3026700)[0m         "torch_dtype": null,
[36m(Runner pid=3026700)[0m         "fsdp_size": -1,
[36m(Runner pid=3026700)[0m         "mp_param_dtype": "bf16",
[36m(Runner pid=3026700)[0m         "mp_reduce_dtype": "fp32",
Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:01,  1.58it/s]
Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:01,  1.34it/s]
Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:01,  1.38it/s]
Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:01,  1.34it/s]
Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:01<00:00,  1.50it/s]
Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  2.43it/s]
Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  2.10it/s]

INFO 12-01 08:11:41 [default_loader.py:272] Loading weights took 1.43 seconds
INFO 12-01 08:11:42 [gpu_model_runner.py:1624] Model loading took 7.5532 GiB and 1.751146 seconds
Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:01<00:00,  1.27it/s]
Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:01<00:00,  1.30it/s]
Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  2.09it/s]
Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  1.79it/s]

INFO 12-01 08:11:42 [default_loader.py:272] Loading weights took 1.68 seconds
Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:01<00:00,  1.25it/s]
Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  2.14it/s]
Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  1.84it/s]

INFO 12-01 08:11:42 [default_loader.py:272] Loading weights took 1.64 seconds
Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  2.03it/s]
Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  1.76it/s]

INFO 12-01 08:11:42 [default_loader.py:272] Loading weights took 1.71 seconds
INFO 12-01 08:11:42 [gpu_model_runner.py:1624] Model loading took 7.5532 GiB and 1.894135 seconds
INFO 12-01 08:11:43 [gpu_model_runner.py:1624] Model loading took 7.5532 GiB and 1.857555 seconds
INFO 12-01 08:11:43 [gpu_model_runner.py:1624] Model loading took 7.5532 GiB and 1.935641 seconds
INFO 12-01 08:11:51 [backends.py:460] vLLM's torch.compile cache is disabled.
INFO 12-01 08:11:51 [backends.py:472] Dynamo bytecode transform time: 8.46 s
INFO 12-01 08:11:51 [backends.py:161] Cache the graph of shape None for later use
INFO 12-01 08:11:51 [backends.py:460] vLLM's torch.compile cache is disabled.
INFO 12-01 08:11:51 [backends.py:472] Dynamo bytecode transform time: 8.48 s
INFO 12-01 08:11:51 [backends.py:460] vLLM's torch.compile cache is disabled.
INFO 12-01 08:11:51 [backends.py:472] Dynamo bytecode transform time: 8.55 s
INFO 12-01 08:11:51 [backends.py:460] vLLM's torch.compile cache is disabled.
INFO 12-01 08:11:51 [backends.py:472] Dynamo bytecode transform time: 8.62 s
INFO 12-01 08:11:52 [backends.py:161] Cache the graph of shape None for later use
INFO 12-01 08:11:52 [backends.py:161] Cache the graph of shape None for later use
INFO 12-01 08:11:52 [backends.py:161] Cache the graph of shape None for later use
INFO 12-01 08:11:58 [backends.py:173] Compiling a graph for general shape takes 7.28 s
[36m(WorkerDict pid=3027760)[0m Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
[36m(WorkerDict pid=3028349)[0m [rank2]:[W1201 08:11:58.726331190 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 2]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
[36m(WorkerDict pid=3027760)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 47.52it/s]
INFO 12-01 08:11:59 [backends.py:173] Compiling a graph for general shape takes 7.29 s
INFO 12-01 08:11:59 [backends.py:173] Compiling a graph for general shape takes 7.46 s
INFO 12-01 08:11:59 [backends.py:173] Compiling a graph for general shape takes 7.45 s
[36m(Runner pid=3026700)[0m         "mp_buffer_dtype": "fp32"
[36m(Runner pid=3026700)[0m       },
[36m(Runner pid=3026700)[0m       "offload": {
[36m(Runner pid=3026700)[0m         "offload_params": true,
[36m(Runner pid=3026700)[0m         "offload_optimizer": false
[36m(Runner pid=3026700)[0m       },
[36m(Runner pid=3026700)[0m       "micro_batch_size_per_device_for_experience": 8,
[36m(Runner pid=3026700)[0m       "padding_free": true,
[36m(Runner pid=3026700)[0m       "ulysses_sequence_parallel_size": 1,
[36m(Runner pid=3026700)[0m       "use_torch_compile": true
[36m(Runner pid=3026700)[0m     },
[36m(Runner pid=3026700)[0m     "reward": {
[36m(Runner pid=3026700)[0m       "reward_type": "batch",
[36m(Runner pid=3026700)[0m       "reward_function": "/data/yexuyan/SPICE/examples/reward_function/caller_penalty.py",
[36m(Runner pid=3026700)[0m       "reward_function_kwargs": {},
[36m(Runner pid=3026700)[0m       "skip_special_tokens": true,
[36m(Runner pid=3026700)[0m       "num_cpus": 1,
[36m(Runner pid=3026700)[0m       "reward_function_name": "compute_score"
[36m(Runner pid=3026700)[0m     },
[36m(Runner pid=3026700)[0m     "rollout": {
[36m(Runner pid=3026700)[0m       "name": "vllm",
[36m(Runner pid=3026700)[0m       "n": 4,
[36m(Runner pid=3026700)[0m       "temperature": 1.0,
[36m(Runner pid=3026700)[0m       "top_p": 0.99,
[36m(Runner pid=3026700)[0m       "top_k": -1,
[36m(Runner pid=3026700)[0m       "seed": 1,
[36m(Runner pid=3026700)[0m       "limit_images": 0,
[36m(Runner pid=3026700)[0m       "dtype": "bf16",
[36m(Runner pid=3026700)[0m       "gpu_memory_utilization": 0.5,
[36m(Runner pid=3026700)[0m       "ignore_eos": false,
[36m(Runner pid=3026700)[0m       "enforce_eager": false,
[36m(Runner pid=3026700)[0m       "enable_chunked_prefill": false,
[36m(Runner pid=3026700)[0m       "tensor_parallel_size": 2,
[36m(Runner pid=3026700)[0m       "max_model_len": null,
[36m(Runner pid=3026700)[0m       "max_num_batched_tokens": 8192,
[36m(Runner pid=3026700)[0m       "disable_log_stats": true,
[36m(Runner pid=3026700)[0m       "val_override_config": {
[36m(Runner pid=3026700)[0m         "temperature": 1.0,
[36m(Runner pid=3026700)[0m         "n": 1
[36m(Runner pid=3026700)[0m       },
[36m(Runner pid=3026700)[0m       "prompt_length": 4096,
[36m(Runner pid=3026700)[0m       "response_length": 2048,
[36m(Runner pid=3026700)[0m       "trust_remote_code": false
[36m(Runner pid=3026700)[0m     }
[36m(Runner pid=3026700)[0m   },
[36m(Runner pid=3026700)[0m   "algorithm": {
[36m(Runner pid=3026700)[0m     "gamma": 1.0,
[36m(Runner pid=3026700)[0m     "lam": 1.0,
[36m(Runner pid=3026700)[0m     "adv_estimator": "grpo",
[36m(Runner pid=3026700)[0m     "disable_kl": false,
[36m(Runner pid=3026700)[0m     "use_kl_loss": true,
[36m(Runner pid=3026700)[0m     "kl_penalty": "low_var_kl",
[36m(Runner pid=3026700)[0m     "kl_coef": 0.01,
[36m(Runner pid=3026700)[0m     "kl_type": "fixed",
[36m(Runner pid=3026700)[0m     "kl_horizon": 0.0,
[36m(Runner pid=3026700)[0m     "kl_target": 0.0,
[36m(Runner pid=3026700)[0m     "mock_data": "test"
[36m(Runner pid=3026700)[0m   },
[36m(Runner pid=3026700)[0m   "trainer": {
[36m(Runner pid=3026700)[0m     "total_epochs": 1000,
[36m(Runner pid=3026700)[0m     "max_steps": 15,
[36m(Runner pid=3026700)[0m     "project_name": "SPICE12-Qwen3-4b-Base",
[36m(Runner pid=3026700)[0m     "experiment_name": "myrun12_questioner_v1",
[36m(Runner pid=3026700)[0m     "logger": [
[36m(Runner pid=3026700)[0m       "console",
[36m(Runner pid=3026700)[0m       "swanlab"
[36m(Runner pid=3026700)[0m     ],
[36m(Runner pid=3026700)[0m     "nnodes": 1,
[36m(Runner pid=3026700)[0m     "n_gpus_per_node": 4,
[36m(Runner pid=3026700)[0m     "critic_warmup": 0,
[36m(Runner pid=3026700)[0m     "val_freq": -1,
[36m(Runner pid=3026700)[0m     "val_before_train": false,
[36m(Runner pid=3026700)[0m     "val_only": false,
[36m(Runner pid=3026700)[0m     "val_generations_to_log": 3,
[36m(Runner pid=3026700)[0m     "save_freq": 5,
[36m(Runner pid=3026700)[0m     "save_limit": 3,
[36m(Runner pid=3026700)[0m     "save_checkpoint_path": "/data/yexuyan/SPICE/storage/models/myrun12_questioner_v1",
[36m(Runner pid=3026700)[0m     "load_checkpoint_path": null
[36m(Runner pid=3026700)[0m   }
[36m(Runner pid=3026700)[0m }
[36m(Runner pid=3026700)[0m Size of train dataloader: 19
[36m(Runner pid=3026700)[0m Size of val dataloader: 10
[36m(Runner pid=3026700)[0m Total training steps: 15
[36m(BatchFunctionRewardManager pid=3027351)[0m Using reward function `compute_score` from `/data/yexuyan/SPICE/examples/reward_function/caller_penalty.py`.
[36m(pid=3027760)[0m WARNING 12-01 08:11:48 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
[36m(BatchFunctionRewardManager pid=3027353)[0m Using reward function `compute_score` from `/data/yexuyan/SPICE/examples/reward_function/caller_penalty.py`.
[36m(pid=3028348)[0m WARNING 12-01 08:11:55 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
[36m(pid=3028349)[0m WARNING 12-01 08:11:55 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
[36m(WorkerDict pid=3027760)[0m actor will use global batch size 64.
[36m(WorkerDict pid=3027760)[0m Model config: Qwen3Config {
[36m(WorkerDict pid=3027760)[0m   "architectures": [
[36m(WorkerDict pid=3027760)[0m     "Qwen3ForCausalLM"
[36m(WorkerDict pid=3027760)[0m   ],
[36m(WorkerDict pid=3027760)[0m   "attention_bias": false,
[36m(WorkerDict pid=3027760)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3027760)[0m   "eos_token_id": 151643,
[36m(WorkerDict pid=3027760)[0m   "head_dim": 128,
[36m(WorkerDict pid=3027760)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3027760)[0m   "hidden_size": 2560,
[36m(WorkerDict pid=3027760)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3027760)[0m   "intermediate_size": 9728,
[36m(WorkerDict pid=3027760)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=3027760)[0m   "max_window_layers": 36,
[36m(WorkerDict pid=3027760)[0m   "model_type": "qwen3",
[36m(WorkerDict pid=3027760)[0m   "num_attention_heads": 32,
[36m(WorkerDict pid=3027760)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=3027760)[0m   "num_key_value_heads": 8,
[36m(WorkerDict pid=3027760)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3027760)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3027760)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3027760)[0m   "rope_theta": 1000000,
[36m(WorkerDict pid=3027760)[0m   "sliding_window": null,
[36m(WorkerDict pid=3027760)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=3027760)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3027760)[0m   "transformers_version": "4.52.4",
[36m(WorkerDict pid=3027760)[0m   "use_cache": true,
[36m(WorkerDict pid=3027760)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3027760)[0m   "vocab_size": 151936
[36m(WorkerDict pid=3027760)[0m }
[36m(WorkerDict pid=3027760)[0m 
[36m(WorkerDict pid=3027760)[0m Ulysses patch applied!
[36m(WorkerDict pid=3027760)[0m NCCL version 2.26.2+cuda12.2
[36m(WorkerDict pid=3027760)[0m Qwen3ForCausalLM contains 4.02B parameters.
[36m(WorkerDict pid=3027760)[0m After huggingface model init: 1.01 GB / 79.33 GB.
[36m(WorkerDict pid=3027760)[0m FSDP wrap policy: functools.partial(<function transformer_auto_wrap_policy at 0x7f1e4432f010>, transformer_layer_cls={<class 'transformers.models.qwen3.modeling_qwen3.Qwen3DecoderLayer'>}).
[36m(WorkerDict pid=3027760)[0m After FSDP module init: 2.90 GB / 79.33 GB.
INFO 12-01 08:12:02 [monitor.py:34] torch.compile takes 15.74 s in total
INFO 12-01 08:12:03 [monitor.py:34] torch.compile takes 15.77 s in total
INFO 12-01 08:12:03 [monitor.py:34] torch.compile takes 16.01 s in total
INFO 12-01 08:12:03 [monitor.py:34] torch.compile takes 16.07 s in total
INFO 12-01 08:12:03 [gpu_worker.py:227] Available KV cache memory: 50.29 GiB
INFO 12-01 08:12:03 [kv_cache_utils.py:715] GPU KV cache size: 366,192 tokens
INFO 12-01 08:12:03 [kv_cache_utils.py:719] Maximum concurrency for 32,768 tokens per request: 11.18x
INFO 12-01 08:12:04 [gpu_worker.py:227] Available KV cache memory: 50.29 GiB
INFO 12-01 08:12:04 [gpu_worker.py:227] Available KV cache memory: 50.29 GiB
INFO 12-01 08:12:04 [kv_cache_utils.py:715] GPU KV cache size: 366,192 tokens
INFO 12-01 08:12:04 [kv_cache_utils.py:719] Maximum concurrency for 32,768 tokens per request: 11.18x
INFO 12-01 08:12:04 [gpu_worker.py:227] Available KV cache memory: 50.29 GiB
INFO 12-01 08:12:04 [kv_cache_utils.py:715] GPU KV cache size: 366,192 tokens
INFO 12-01 08:12:04 [kv_cache_utils.py:719] Maximum concurrency for 32,768 tokens per request: 11.18x
INFO 12-01 08:12:05 [kv_cache_utils.py:715] GPU KV cache size: 366,192 tokens
INFO 12-01 08:12:05 [kv_cache_utils.py:719] Maximum concurrency for 32,768 tokens per request: 11.18x
[36m(WorkerDict pid=3027760)[0m Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
[36m(WorkerDict pid=3027760)[0m [rank0]:[W1201 08:11:58.983576778 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=3027760)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 49.53it/s]
INFO 12-01 08:12:26 [gpu_model_runner.py:2048] Graph capturing finished in 23 secs, took 2.12 GiB
INFO 12-01 08:12:26 [core.py:171] init engine (profile, create kv cache, warmup model) took 44.49 seconds
[idle_worker] GPU idle worker started.
 * Serving Flask app 'start_vllm_server'
 * Debug mode: off
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on http://127.0.0.1:5002
Press CTRL+C to quit
INFO 12-01 08:12:28 [gpu_model_runner.py:2048] Graph capturing finished in 24 secs, took 2.12 GiB
INFO 12-01 08:12:28 [core.py:171] init engine (profile, create kv cache, warmup model) took 45.26 seconds
INFO 12-01 08:12:28 [gpu_model_runner.py:2048] Graph capturing finished in 24 secs, took 2.12 GiB
INFO 12-01 08:12:28 [core.py:171] init engine (profile, create kv cache, warmup model) took 45.70 seconds
[idle_worker] GPU idle worker started.
 * Serving Flask app 'start_vllm_server'
 * Debug mode: off
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on http://127.0.0.1:5003
Press CTRL+C to quit
INFO 12-01 08:12:28 [gpu_model_runner.py:2048] Graph capturing finished in 24 secs, took 2.12 GiB
INFO 12-01 08:12:28 [core.py:171] init engine (profile, create kv cache, warmup model) took 45.77 seconds
[idle_worker] GPU idle worker started.
 * Serving Flask app 'start_vllm_server'
 * Debug mode: off
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on http://127.0.0.1:5000
Press CTRL+C to quit
[idle_worker] GPU idle worker started.
 * Serving Flask app 'start_vllm_server'
 * Debug mode: off
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on http://127.0.0.1:5001
Press CTRL+C to quit
[36m(pid=3028350)[0m WARNING 12-01 08:11:55 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
[36m(WorkerDict pid=3027760)[0m After offload ref model during init: 1.02 GB / 79.33 GB.
[36m(WorkerDict pid=3027760)[0m Model config: Qwen3Config {
[36m(WorkerDict pid=3027760)[0m   "architectures": [
[36m(WorkerDict pid=3027760)[0m     "Qwen3ForCausalLM"
[36m(WorkerDict pid=3027760)[0m   ],
[36m(WorkerDict pid=3027760)[0m   "attention_bias": false,
[36m(WorkerDict pid=3027760)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3027760)[0m   "eos_token_id": 151643,
[36m(WorkerDict pid=3027760)[0m   "head_dim": 128,
[36m(WorkerDict pid=3027760)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3027760)[0m   "hidden_size": 2560,
[36m(WorkerDict pid=3027760)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3027760)[0m   "intermediate_size": 9728,
[36m(WorkerDict pid=3027760)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=3027760)[0m   "max_window_layers": 36,
[36m(WorkerDict pid=3027760)[0m   "model_type": "qwen3",
[36m(WorkerDict pid=3027760)[0m   "num_attention_heads": 32,
[36m(WorkerDict pid=3027760)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=3027760)[0m   "num_key_value_heads": 8,
[36m(WorkerDict pid=3027760)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3027760)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3027760)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3027760)[0m   "rope_theta": 1000000,
[36m(WorkerDict pid=3027760)[0m   "sliding_window": null,
[36m(WorkerDict pid=3027760)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=3027760)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3027760)[0m   "transformers_version": "4.52.4",
[36m(WorkerDict pid=3027760)[0m   "use_cache": true,
[36m(WorkerDict pid=3027760)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3027760)[0m   "vocab_size": 151936
[36m(WorkerDict pid=3027760)[0m }
[36m(WorkerDict pid=3027760)[0m 
[36m(WorkerDict pid=3027760)[0m Ulysses patch applied!
[36m(WorkerDict pid=3027760)[0m Qwen3ForCausalLM contains 4.02B parameters.
[36m(WorkerDict pid=3027760)[0m After huggingface model init: 1.02 GB / 79.33 GB.
[36m(WorkerDict pid=3027760)[0m FSDP wrap policy: functools.partial(<function transformer_auto_wrap_policy at 0x7f1e4432f010>, transformer_layer_cls={<class 'transformers.models.qwen3.modeling_qwen3.Qwen3DecoderLayer'>}).
[36m(WorkerDict pid=3027760)[0m After FSDP module init: 8.21 GB / 79.33 GB.
[36m(WorkerDict pid=3027760)[0m After optimizer init: 8.21 GB / 79.33 GB.
[36m(WorkerDict pid=3027760)[0m After offload actor model during init: 1.03 GB / 79.33 GB.
[36m(WorkerDict pid=3027760)[0m After offload actor optimizer during init: 1.03 GB / 79.33 GB.
[36m(WorkerDict pid=3028350)[0m WARNING 12-01 08:12:26 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ef8eab6d7b0>
[36m(WorkerDict pid=3028349)[0m NCCL version 2.26.2+cuda12.2
[36m(WorkerDict pid=3027760)[0m WARNING 12-01 08:12:27 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[36m(WorkerDict pid=3028349)[0m Sampling params: {'max_tokens': 2048, 'detokenize': False, 'logit_bias': None, 'n': 4, 'temperature': 1.0, 'top_p': 0.99, 'top_k': -1, 'ignore_eos': False}.
[36m(WorkerDict pid=3027760)[0m WARNING 12-01 08:12:27 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7eed147e5b40>[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3028350)[0m WARNING 12-01 08:12:27 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3027760)[0m After vllm init: 4.31 GB / 79.33 GB.
[36m(Runner pid=3026700)[0m Config
[36m(Runner pid=3026700)[0m algorithm:
[36m(Runner pid=3026700)[0m   adv_estimator: grpo
[36m(Runner pid=3026700)[0m   disable_kl: false
[36m(Runner pid=3026700)[0m   gamma: 1.0
[36m(Runner pid=3026700)[0m   kl_coef: 0.01
[36m(Runner pid=3026700)[0m   kl_horizon: 0.0
[36m(Runner pid=3026700)[0m   kl_penalty: low_var_kl
[36m(Runner pid=3026700)[0m   kl_target: 0.0
[36m(Runner pid=3026700)[0m   kl_type: fixed
[36m(Runner pid=3026700)[0m   lam: 1.0
[36m(Runner pid=3026700)[0m   mock_data: test
[36m(Runner pid=3026700)[0m   use_kl_loss: true
[36m(Runner pid=3026700)[0m data:
[36m(Runner pid=3026700)[0m   answer_key: id
[36m(Runner pid=3026700)[0m   context_key: text
[36m(Runner pid=3026700)[0m   filter_overlong_prompts: false
[36m(Runner pid=3026700)[0m   format_prompt: null
[36m(Runner pid=3026700)[0m   image_key: images
[36m(Runner pid=3026700)[0m   max_pixels: 4194304
[36m(Runner pid=3026700)[0m   max_prompt_length: 4096
[36m(Runner pid=3026700)[0m   max_response_length: 2048
[36m(Runner pid=3026700)[0m   min_pixels: 262144
[36m(Runner pid=3026700)[0m   override_chat_template: null
[36m(Runner pid=3026700)[0m   prompt_key: text
[36m(Runner pid=3026700)[0m   rollout_batch_size: 512
[36m(Runner pid=3026700)[0m   seed: 1
[36m(Runner pid=3026700)[0m   shuffle: true
[36m(Runner pid=3026700)[0m   train_files: /data/yexuyan/SPICE/storage/datasets
[36m(Runner pid=3026700)[0m   val_batch_size: 1024
[36m(Runner pid=3026700)[0m   val_files: /data/yexuyan/SPICE/storage/datasets
[36m(Runner pid=3026700)[0m trainer:
[36m(Runner pid=3026700)[0m   critic_warmup: 0
[36m(Runner pid=3026700)[0m   experiment_name: myrun12_questioner_v1
[36m(Runner pid=3026700)[0m   load_checkpoint_path: null
[36m(Runner pid=3026700)[0m   logger:
[36m(Runner pid=3026700)[0m   - console
[36m(Runner pid=3026700)[0m   - swanlab
[36m(Runner pid=3026700)[0m   max_steps: 15
[36m(Runner pid=3026700)[0m   n_gpus_per_node: 4
[36m(Runner pid=3026700)[0m   nnodes: 1
[36m(Runner pid=3026700)[0m   project_name: SPICE12-Qwen3-4b-Base
[36m(Runner pid=3026700)[0m   save_checkpoint_path: /data/yexuyan/SPICE/storage/models/myrun12_questioner_v1
[36m(Runner pid=3026700)[0m   save_freq: 5
[36m(Runner pid=3026700)[0m   save_limit: 3
[36m(Runner pid=3026700)[0m   total_epochs: 1000
[36m(Runner pid=3026700)[0m   val_before_train: false
[36m(Runner pid=3026700)[0m   val_freq: -1
[36m(Runner pid=3026700)[0m   val_generations_to_log: 3
[36m(Runner pid=3026700)[0m   val_only: false
[36m(Runner pid=3026700)[0m worker:
[36m(Runner pid=3026700)[0m   actor:
[36m(Runner pid=3026700)[0m     clip_ratio_dual: 3.0
[36m(Runner pid=3026700)[0m     clip_ratio_high: 0.3
[36m(Runner pid=3026700)[0m     clip_ratio_low: 0.2
[36m(Runner pid=3026700)[0m     disable_kl: false
[36m(Runner pid=3026700)[0m     fsdp:
[36m(Runner pid=3026700)[0m       enable_cpu_offload: false
[36m(Runner pid=3026700)[0m       enable_full_shard: true
[36m(Runner pid=3026700)[0m       enable_rank0_init: true
[36m(Runner pid=3026700)[0m       fsdp_size: -1
[36m(Runner pid=3026700)[0m       mp_buffer_dtype: fp32
[36m(Runner pid=3026700)[0m       mp_param_dtype: bf16
[36m(Runner pid=3026700)[0m       mp_reduce_dtype: fp32
[36m(Runner pid=3026700)[0m       torch_dtype: null
[36m(Runner pid=3026700)[0m       use_orig_params: false
[36m(Runner pid=3026700)[0m     global_batch_size: 16
[36m(Runner pid=3026700)[0m     global_batch_size_per_device: -1
[36m(Runner pid=3026700)[0m     kl_coef: 0.01
[36m(Runner pid=3026700)[0m     kl_penalty: low_var_kl
[36m(Runner pid=3026700)[0m     max_grad_norm: 1.0
[36m(Runner pid=3026700)[0m     micro_batch_size_per_device_for_experience: 8
[36m(Runner pid=3026700)[0m     micro_batch_size_per_device_for_update: 2
[36m(Runner pid=3026700)[0m     model:
[36m(Runner pid=3026700)[0m       enable_gradient_checkpointing: true
[36m(Runner pid=3026700)[0m       freeze_vision_tower: false
[36m(Runner pid=3026700)[0m       model_path: /data/yexuyan/SPICE/storage/models/Qwen3-4B-Base
[36m(Runner pid=3026700)[0m       override_config: {}
[36m(Runner pid=3026700)[0m       tokenizer_path: /data/yexuyan/SPICE/storage/models/Qwen3-4B-Base
[36m(Runner pid=3026700)[0m       trust_remote_code: false
[36m(Runner pid=3026700)[0m     offload:
[36m(Runner pid=3026700)[0m       offload_optimizer: true
[36m(Runner pid=3026700)[0m       offload_params: true
[36m(Runner pid=3026700)[0m     optim:
[36m(Runner pid=3026700)[0m       betas:
[36m(Runner pid=3026700)[0m       - 0.9
[36m(Runner pid=3026700)[0m       - 0.999
[36m(Runner pid=3026700)[0m       lr: 1.0e-06
[36m(Runner pid=3026700)[0m       lr_warmup_ratio: 0.0
[36m(Runner pid=3026700)[0m       min_lr_ratio: null
[36m(Runner pid=3026700)[0m       strategy: adamw
[36m(Runner pid=3026700)[0m       training_steps: 15
[36m(Runner pid=3026700)[0m       warmup_style: constant
[36m(Runner pid=3026700)[0m       weight_decay: 0.01
[36m(Runner pid=3026700)[0m     padding_free: true
[36m(Runner pid=3026700)[0m     ppo_epochs: 1
[36m(Runner pid=3026700)[0m     strategy: fsdp
[36m(Runner pid=3026700)[0m     ulysses_sequence_parallel_size: 1
[36m(Runner pid=3026700)[0m     use_kl_loss: true
[36m(Runner pid=3026700)[0m     use_torch_compile: true
[36m(Runner pid=3026700)[0m   critic:
[36m(Runner pid=3026700)[0m     cliprange_value: 0.5
[36m(Runner pid=3026700)[0m     fsdp:
[36m(Runner pid=3026700)[0m       enable_cpu_offload: false
[36m(Runner pid=3026700)[0m       enable_full_shard: true
[36m(Runner pid=3026700)[0m       enable_rank0_init: false
[36m(Runner pid=3026700)[0m       fsdp_size: -1
[36m(Runner pid=3026700)[0m       mp_buffer_dtype: fp32
[36m(Runner pid=3026700)[0m       mp_param_dtype: bf16
[36m(Runner pid=3026700)[0m       mp_reduce_dtype: fp32
[36m(Runner pid=3026700)[0m       torch_dtype: null
[36m(Runner pid=3026700)[0m       use_orig_params: false
[36m(Runner pid=3026700)[0m     global_batch_size: 256
[36m(Runner pid=3026700)[0m     global_batch_size_per_device: -1
[36m(Runner pid=3026700)[0m     max_grad_norm: 1.0
[36m(Runner pid=3026700)[0m     micro_batch_size_per_device_for_experience: 16
[36m(Runner pid=3026700)[0m     micro_batch_size_per_device_for_update: 4
[36m(Runner pid=3026700)[0m     model:
[36m(Runner pid=3026700)[0m       enable_gradient_checkpointing: true
[36m(Runner pid=3026700)[0m       freeze_vision_tower: false
[36m(Runner pid=3026700)[0m       model_path: null
[36m(Runner pid=3026700)[0m       override_config: {}
[36m(Runner pid=3026700)[0m       tokenizer_path: null
[36m(Runner pid=3026700)[0m       trust_remote_code: true
[36m(Runner pid=3026700)[0m     offload:
[36m(Runner pid=3026700)[0m       offload_optimizer: false
[36m(Runner pid=3026700)[0m       offload_params: false
[36m(Runner pid=3026700)[0m     optim:
[36m(Runner pid=3026700)[0m       betas:
[36m(Runner pid=3026700)[0m       - 0.9
[36m(Runner pid=3026700)[0m       - 0.999
[36m(Runner pid=3026700)[0m       lr: 1.0e-06
[36m(Runner pid=3026700)[0m       lr_warmup_ratio: 0.0
[36m(Runner pid=3026700)[0m       min_lr_ratio: null
[36m(Runner pid=3026700)[0m       strategy: adamw
[36m(Runner pid=3026700)[0m       training_steps: 15
[36m(Runner pid=3026700)[0m       warmup_style: constant
[36m(Runner pid=3026700)[0m       weight_decay: 0.01
[36m(Runner pid=3026700)[0m     padding_free: false
[36m(Runner pid=3026700)[0m     ppo_epochs: 1
[36m(Runner pid=3026700)[0m     strategy: fsdp
[36m(Runner pid=3026700)[0m     ulysses_sequence_parallel_size: 1
[36m(Runner pid=3026700)[0m   hybrid_engine: true
[36m(Runner pid=3026700)[0m   ref:
[36m(Runner pid=3026700)[0m     fsdp:
[36m(Runner pid=3026700)[0m       enable_cpu_offload: true
[36m(Runner pid=3026700)[0m       enable_full_shard: true
[36m(Runner pid=3026700)[0m       enable_rank0_init: true
[36m(Runner pid=3026700)[0m       fsdp_size: -1
[36m(Runner pid=3026700)[0m       mp_buffer_dtype: fp32
[36m(Runner pid=3026700)[0m       mp_param_dtype: bf16
[36m(Runner pid=3026700)[0m       mp_reduce_dtype: fp32
[36m(Runner pid=3026700)[0m       torch_dtype: null
[36m(Runner pid=3026700)[0m       use_orig_params: false
[36m(Runner pid=3026700)[0m     micro_batch_size_per_device_for_experience: 8
[36m(Runner pid=3026700)[0m     offload:
[36m(Runner pid=3026700)[0m       offload_optimizer: false
[36m(Runner pid=3026700)[0m       offload_params: true
[36m(Runner pid=3026700)[0m     padding_free: true
[36m(Runner pid=3026700)[0m     strategy: fsdp
[36m(Runner pid=3026700)[0m     ulysses_sequence_parallel_size: 1
[36m(Runner pid=3026700)[0m     use_torch_compile: true
[36m(Runner pid=3026700)[0m   reward:
[36m(Runner pid=3026700)[0m     num_cpus: 1
[36m(Runner pid=3026700)[0m     reward_function: /data/yexuyan/SPICE/examples/reward_function/caller_penalty.py
[36m(Runner pid=3026700)[0m     reward_function_kwargs: {}
[36m(Runner pid=3026700)[0m     reward_function_name: compute_score
[36m(Runner pid=3026700)[0m     reward_type: batch
[36m(Runner pid=3026700)[0m     skip_special_tokens: true
[36m(Runner pid=3026700)[0m   rollout:
[36m(Runner pid=3026700)[0m     disable_log_stats: true
[36m(Runner pid=3026700)[0m     dtype: bf16
[36m(Runner pid=3026700)[0m     enable_chunked_prefill: false
[36m(Runner pid=3026700)[0m     enforce_eager: false
[36m(Runner pid=3026700)[0m     gpu_memory_utilization: 0.5
[36m(Runner pid=3026700)[0m     ignore_eos: false
[36m(Runner pid=3026700)[0m     limit_images: 0
[36m(Runner pid=3026700)[0m     max_model_len: null
[36m(Runner pid=3026700)[0m     max_num_batched_tokens: 8192
[36m(Runner pid=3026700)[0m     n: 4
[36m(Runner pid=3026700)[0m     name: vllm
[36m(Runner pid=3026700)[0m     prompt_length: 4096
[36m(Runner pid=3026700)[0m     response_length: 2048
[36m(Runner pid=3026700)[0m     seed: 1
[36m(Runner pid=3026700)[0m     temperature: 1.0
[36m(Runner pid=3026700)[0m     tensor_parallel_size: 2
[36m(Runner pid=3026700)[0m     top_k: -1
[36m(Runner pid=3026700)[0m     top_p: 0.99
[36m(Runner pid=3026700)[0m     trust_remote_code: false
[36m(Runner pid=3026700)[0m     val_override_config:
[36m(Runner pid=3026700)[0m       n: 1
[36m(Runner pid=3026700)[0m       temperature: 1.0
[36m(Runner pid=3026700)[0m 
[36m(Runner pid=3026700)[0m swanlab: Tracking run with swanlab version 0.6.13
[36m(Runner pid=3026700)[0m swanlab: Run data will be saved locally in 
[36m(Runner pid=3026700)[0m /data/yexuyan/SPICE/swanlab_log/run-20251201_081409-g47qyae921grh5igverji
[36m(Runner pid=3026700)[0m swanlab: ðŸ‘‹ Hi lulacola,welcome to swanlab!
[36m(Runner pid=3026700)[0m swanlab: Syncing run myrun12_questioner_v1 to the cloud
[36m(Runner pid=3026700)[0m swanlab: ðŸ  View project at https://swanlab.cn/@lulacola/SPICE12-Qwen3-4b-Base
[36m(Runner pid=3026700)[0m swanlab: ðŸš€ View run at 
[36m(Runner pid=3026700)[0m https://swanlab.cn/@lulacola/SPICE12-Qwen3-4b-Base/runs/g47qyae921grh5igverji
[2m[36m(pid=3026700) [0mEpoch 0:   0%|          | 0.00/1.00k [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/yexuyan/miniconda3/envs/r0/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/yexuyan/miniconda3/envs/r0/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/data/yexuyan/SPICE/verl/trainer/main.py", line 133, in <module>
    main()
  File "/data/yexuyan/SPICE/verl/trainer/main.py", line 129, in main
    ray.get(runner.run.remote(ppo_config))
  File "/home/yexuyan/miniconda3/envs/r0/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/yexuyan/miniconda3/envs/r0/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/yexuyan/miniconda3/envs/r0/lib/python3.10/site-packages/ray/_private/worker.py", line 2822, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/yexuyan/miniconda3/envs/r0/lib/python3.10/site-packages/ray/_private/worker.py", line 930, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(AttributeError): [36mray::Runner.run()[39m (pid=3026700, ip=172.16.0.32, actor_id=3e44ee0143eb1287d3cf0f9701000000, repr=<main.Runner object at 0x7f5be9d43a30>)
  File "/data/yexuyan/SPICE/verl/trainer/main.py", line 96, in run
    trainer.fit()
  File "/data/yexuyan/SPICE/verl/trainer/ray_trainer.py", line 484, in fit
    for batch_dict in tqdm(self.train_dataloader, desc="Running step", position=1):
  File "/home/yexuyan/miniconda3/envs/r0/lib/python3.10/site-packages/ray/experimental/tqdm_ray.py", line 159, in __iter__
    for x in iter(self._iterable):
  File "/home/yexuyan/miniconda3/envs/r0/lib/python3.10/site-packages/torchdata/stateful_dataloader/stateful_dataloader.py", line 450, in __next__
    return super().__next__()
  File "/home/yexuyan/miniconda3/envs/r0/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 733, in __next__
    data = self._next_data()
  File "/home/yexuyan/miniconda3/envs/r0/lib/python3.10/site-packages/torchdata/stateful_dataloader/stateful_dataloader.py", line 1456, in _next_data
    return self._process_data(data, worker_id, state_dict)
  File "/home/yexuyan/miniconda3/envs/r0/lib/python3.10/site-packages/torchdata/stateful_dataloader/stateful_dataloader.py", line 1543, in _process_data
    data.reraise()
  File "/home/yexuyan/miniconda3/envs/r0/lib/python3.10/site-packages/torch/_utils.py", line 750, in reraise
    raise exception
AttributeError: Caught AttributeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/yexuyan/miniconda3/envs/r0/lib/python3.10/site-packages/torchdata/stateful_dataloader/worker.py", line 242, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[union-attr]
  File "/home/yexuyan/miniconda3/envs/r0/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/yexuyan/miniconda3/envs/r0/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/data/yexuyan/SPICE/verl/utils/dataset.py", line 183, in __getitem__
    messages = self._build_messages(example)
  File "/data/yexuyan/SPICE/verl/utils/dataset.py", line 136, in _build_messages
    if not self.use_free_form_challenger:
AttributeError: 'RLHFDataset' object has no attribute 'use_free_form_challenger'
                                                                   [2m[36m(pid=3026700) [0mEpoch 0:   0%|          | 1.00/1.00k [00:00<10:50, 1.54it/s][36m(WorkerDict pid=3028348)[0m Sampling params: {'max_tokens': 2048, 'detokenize': False, 'logit_bias': None, 'n': 4, 'temperature': 1.0, 'top_p': 0.99, 'top_k': -1, 'ignore_eos': False}.[32m [repeated 3x across cluster][0m
merging model
Traceback (most recent call last):
  File "/data/yexuyan/SPICE/scripts/model_merger.py", line 66, in <module>
    for filename in os.listdir(local_dir):
FileNotFoundError: [Errno 2] No such file or directory: '/data/yexuyan/SPICE/storage/models/myrun12_questioner_v1/global_step_5/actor'
pkill: killing pid 1021580 failed: Operation not permitted
pkill: killing pid 1416387 failed: Operation not permitted
pkill: killing pid 2365089 failed: Operation not permitted
pkill: killing pid 2365400 failed: Operation not permitted
pkill: killing pid 2365401 failed: Operation not permitted
pkill: killing pid 2365404 failed: Operation not permitted
pkill: killing pid 2365405 failed: Operation not permitted
pkill: killing pid 2365406 failed: Operation not permitted
pkill: killing pid 2365407 failed: Operation not permitted
questioner training finished
