nohup: ignoring input
Model_abbr: SPCIE-myrun
/data/yexuyan/SPICE/storage
start train solver SPCIE-myrun_solver_v1 storage/models/Qwen3-4B-Base /data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface
start generate question
INFO 11-28 01:05:12 [__init__.py:244] Automatically detected platform cuda.
INFO 11-28 01:05:12 [__init__.py:244] Automatically detected platform cuda.
INFO 11-28 01:05:12 [__init__.py:244] Automatically detected platform cuda.
INFO 11-28 01:05:12 [__init__.py:244] Automatically detected platform cuda.
INFO 11-28 01:05:12 [__init__.py:244] Automatically detected platform cuda.
INFO 11-28 01:05:12 [__init__.py:244] Automatically detected platform cuda.
INFO 11-28 01:05:12 [__init__.py:244] Automatically detected platform cuda.
INFO 11-28 01:05:12 [__init__.py:244] Automatically detected platform cuda.
INFO 11-28 01:05:23 [config.py:823] This model supports multiple tasks: {'reward', 'classify', 'embed', 'generate', 'score'}. Defaulting to 'generate'.
INFO 11-28 01:05:23 [config.py:823] This model supports multiple tasks: {'classify', 'embed', 'generate', 'score', 'reward'}. Defaulting to 'generate'.
INFO 11-28 01:05:23 [config.py:823] This model supports multiple tasks: {'embed', 'reward', 'score', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 11-28 01:05:23 [config.py:823] This model supports multiple tasks: {'generate', 'score', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.
INFO 11-28 01:05:23 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 11-28 01:05:23 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 11-28 01:05:23 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 11-28 01:05:23 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 11-28 01:05:23 [config.py:823] This model supports multiple tasks: {'embed', 'score', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.
INFO 11-28 01:05:23 [config.py:823] This model supports multiple tasks: {'reward', 'classify', 'embed', 'score', 'generate'}. Defaulting to 'generate'.
INFO 11-28 01:05:23 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 11-28 01:05:23 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 11-28 01:05:23 [config.py:823] This model supports multiple tasks: {'reward', 'score', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.
INFO 11-28 01:05:23 [config.py:823] This model supports multiple tasks: {'classify', 'generate', 'embed', 'score', 'reward'}. Defaulting to 'generate'.
INFO 11-28 01:05:23 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 11-28 01:05:23 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=16384.
INFO 11-28 01:05:24 [core.py:455] Waiting for init message from front-end.
INFO 11-28 01:05:24 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface', speculative_config=None, tokenizer='/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=4, served_model_name=/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 11-28 01:05:24 [core.py:455] Waiting for init message from front-end.
INFO 11-28 01:05:24 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface', speculative_config=None, tokenizer='/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=2, served_model_name=/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 11-28 01:05:24 [core.py:455] Waiting for init message from front-end.
INFO 11-28 01:05:24 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface', speculative_config=None, tokenizer='/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=6, served_model_name=/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 11-28 01:05:24 [core.py:455] Waiting for init message from front-end.
INFO 11-28 01:05:24 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface', speculative_config=None, tokenizer='/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=1, served_model_name=/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
WARNING 11-28 01:05:25 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fa18afaa9b0>
WARNING 11-28 01:05:25 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f87bcbf5b70>
INFO 11-28 01:05:25 [core.py:455] Waiting for init message from front-end.
INFO 11-28 01:05:25 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface', speculative_config=None, tokenizer='/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=5, served_model_name=/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
WARNING 11-28 01:05:25 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fb9ec33ebc0>
WARNING 11-28 01:05:25 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f25afc36980>
INFO 11-28 01:05:25 [core.py:455] Waiting for init message from front-end.
INFO 11-28 01:05:25 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface', speculative_config=None, tokenizer='/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
WARNING 11-28 01:05:25 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f097d861c00>
WARNING 11-28 01:05:25 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7faebf3a9cc0>
INFO 11-28 01:05:25 [core.py:455] Waiting for init message from front-end.
INFO 11-28 01:05:25 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface', speculative_config=None, tokenizer='/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=3, served_model_name=/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 11-28 01:05:25 [core.py:455] Waiting for init message from front-end.
INFO 11-28 01:05:25 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface', speculative_config=None, tokenizer='/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=7, served_model_name=/data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 11-28 01:05:25 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 11-28 01:05:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 11-28 01:05:25 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f90409fdc30>
INFO 11-28 01:05:25 [gpu_model_runner.py:1595] Starting to load model /data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface...
WARNING 11-28 01:05:25 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f5103685cc0>
INFO 11-28 01:05:25 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 11-28 01:05:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 11-28 01:05:25 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 11-28 01:05:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 11-28 01:05:25 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 11-28 01:05:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 11-28 01:05:25 [gpu_model_runner.py:1600] Loading model from scratch...
INFO 11-28 01:05:25 [gpu_model_runner.py:1595] Starting to load model /data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface...
INFO 11-28 01:05:25 [gpu_model_runner.py:1595] Starting to load model /data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface...
INFO 11-28 01:05:25 [gpu_model_runner.py:1595] Starting to load model /data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface...
INFO 11-28 01:05:25 [cuda.py:252] Using Flash Attention backend on V1 engine.
INFO 11-28 01:05:26 [gpu_model_runner.py:1600] Loading model from scratch...
INFO 11-28 01:05:26 [gpu_model_runner.py:1600] Loading model from scratch...
INFO 11-28 01:05:26 [gpu_model_runner.py:1600] Loading model from scratch...
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
INFO 11-28 01:05:26 [cuda.py:252] Using Flash Attention backend on V1 engine.
INFO 11-28 01:05:26 [cuda.py:252] Using Flash Attention backend on V1 engine.
INFO 11-28 01:05:26 [cuda.py:252] Using Flash Attention backend on V1 engine.
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.33it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.15it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.38it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.14it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.30it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.31it/s]

INFO 11-28 01:05:27 [default_loader.py:272] Loading weights took 1.63 seconds
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.34it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.24it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.24it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.34it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.22it/s]


Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.22it/s]

INFO 11-28 01:05:28 [gpu_model_runner.py:1624] Model loading took 7.5532 GiB and 1.883719 seconds
INFO 11-28 01:05:28 [default_loader.py:272] Loading weights took 1.72 seconds
INFO 11-28 01:05:28 [default_loader.py:272] Loading weights took 1.72 seconds
INFO 11-28 01:05:28 [default_loader.py:272] Loading weights took 1.57 seconds
INFO 11-28 01:05:28 [gpu_model_runner.py:1624] Model loading took 7.5532 GiB and 2.028835 seconds
INFO 11-28 01:05:28 [gpu_model_runner.py:1624] Model loading took 7.5532 GiB and 2.030262 seconds
INFO 11-28 01:05:28 [gpu_model_runner.py:1624] Model loading took 7.5532 GiB and 2.026331 seconds
[W1128 01:05:35.980270539 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W1128 01:05:36.157519149 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W1128 01:05:36.379036640 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W1128 01:05:36.395644240 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
INFO 11-28 01:05:36 [backends.py:460] vLLM's torch.compile cache is disabled.
INFO 11-28 01:05:36 [backends.py:472] Dynamo bytecode transform time: 8.31 s
INFO 11-28 01:05:37 [backends.py:161] Cache the graph of shape None for later use
INFO 11-28 01:05:37 [backends.py:460] vLLM's torch.compile cache is disabled.
INFO 11-28 01:05:37 [backends.py:472] Dynamo bytecode transform time: 8.44 s
INFO 11-28 01:05:37 [backends.py:460] vLLM's torch.compile cache is disabled.
INFO 11-28 01:05:37 [backends.py:472] Dynamo bytecode transform time: 8.50 s
INFO 11-28 01:05:37 [backends.py:460] vLLM's torch.compile cache is disabled.
INFO 11-28 01:05:37 [backends.py:472] Dynamo bytecode transform time: 8.56 s
INFO 11-28 01:05:37 [backends.py:161] Cache the graph of shape None for later use
INFO 11-28 01:05:37 [backends.py:161] Cache the graph of shape None for later use
INFO 11-28 01:05:37 [backends.py:161] Cache the graph of shape None for later use
INFO 11-28 01:05:44 [backends.py:173] Compiling a graph for general shape takes 7.24 s
INFO 11-28 01:05:44 [backends.py:173] Compiling a graph for general shape takes 7.28 s
INFO 11-28 01:05:45 [backends.py:173] Compiling a graph for general shape takes 7.43 s
INFO 11-28 01:05:45 [backends.py:173] Compiling a graph for general shape takes 7.43 s
[W1128 01:05:45.990749939 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
INFO 11-28 01:05:46 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 11-28 01:05:46 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 11-28 01:05:46 [gpu_model_runner.py:1595] Starting to load model /data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface...
[W1128 01:05:46.165967501 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
INFO 11-28 01:05:46 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 11-28 01:05:46 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 11-28 01:05:46 [gpu_model_runner.py:1595] Starting to load model /data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface...
INFO 11-28 01:05:46 [gpu_model_runner.py:1600] Loading model from scratch...
INFO 11-28 01:05:46 [cuda.py:252] Using Flash Attention backend on V1 engine.
[W1128 01:05:46.389604743 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W1128 01:05:46.401859291 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
INFO 11-28 01:05:46 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 11-28 01:05:46 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 11-28 01:05:46 [gpu_model_runner.py:1600] Loading model from scratch...
INFO 11-28 01:05:46 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 11-28 01:05:46 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 11-28 01:05:46 [gpu_model_runner.py:1595] Starting to load model /data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface...
INFO 11-28 01:05:46 [gpu_model_runner.py:1595] Starting to load model /data/yexuyan/SPICE/storage/models/SPCIE-myrun_questioner_v1/global_step_20/actor/huggingface...
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
INFO 11-28 01:05:46 [cuda.py:252] Using Flash Attention backend on V1 engine.
INFO 11-28 01:05:46 [gpu_model_runner.py:1600] Loading model from scratch...
INFO 11-28 01:05:46 [gpu_model_runner.py:1600] Loading model from scratch...
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
INFO 11-28 01:05:46 [cuda.py:252] Using Flash Attention backend on V1 engine.
INFO 11-28 01:05:46 [cuda.py:252] Using Flash Attention backend on V1 engine.
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.57it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.60it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.43it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.32it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.51it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.52it/s]

INFO 11-28 01:05:47 [default_loader.py:272] Loading weights took 1.39 seconds
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.58it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.58it/s]

INFO 11-28 01:05:48 [default_loader.py:272] Loading weights took 1.34 seconds
INFO 11-28 01:05:48 [monitor.py:34] torch.compile takes 15.54 s in total
INFO 11-28 01:05:48 [gpu_model_runner.py:1624] Model loading took 7.5532 GiB and 1.616578 seconds
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.34it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.35it/s]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.29it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.29it/s]

INFO 11-28 01:05:48 [gpu_model_runner.py:1624] Model loading took 7.5532 GiB and 1.635805 seconds
INFO 11-28 01:05:48 [default_loader.py:272] Loading weights took 1.57 seconds
INFO 11-28 01:05:48 [default_loader.py:272] Loading weights took 1.65 seconds
INFO 11-28 01:05:48 [monitor.py:34] torch.compile takes 15.73 s in total
INFO 11-28 01:05:48 [monitor.py:34] torch.compile takes 15.93 s in total
INFO 11-28 01:05:48 [gpu_model_runner.py:1624] Model loading took 7.5532 GiB and 1.879414 seconds
INFO 11-28 01:05:49 [monitor.py:34] torch.compile takes 15.99 s in total
INFO 11-28 01:05:49 [gpu_model_runner.py:1624] Model loading took 7.5532 GiB and 1.806882 seconds
INFO 11-28 01:05:49 [gpu_worker.py:227] Available KV cache memory: 58.22 GiB
INFO 11-28 01:05:49 [kv_cache_utils.py:715] GPU KV cache size: 423,952 tokens
INFO 11-28 01:05:49 [kv_cache_utils.py:719] Maximum concurrency for 32,768 tokens per request: 12.94x
